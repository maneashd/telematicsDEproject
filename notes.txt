1)

To check if fake data is sent to kafka broker after main.py is ran in vscode 
>> we need to go to broker in docker desktop and under exec tab give below command
kafka-topics --list --bootstrap-server broker:29092

2)

Below command is to go into the topic an d check the data for that topic
kafka-console-consumer --topic weather_data --bootstrap-server broker:29092 --from-beginning

3)

7077:7077⁠ this is where we are goint to submit out spark jobs
9090:8080⁠ this is where we see our workers in UI

4)

config("spark.jars.packages",
                "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.0,"     #Spark Kafka Integration, also means my spark sql installed will be able to connect to spark kafka
                "org.apache.hadoop:hadoop-aws:3.3.1,"                   #Hadoop AWS Integration
                "com.amazonaws:aws-java-sdk:1.11.469")

these jars and packages we can get from www.mavenrepository.com

5)

docker compose up -d - used to start docker-compose file.

6)
docker-compose exec spark-master spark-submit --master spark://spark-master:7077 --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.hadoop:hadoop-aws:3.3.1,com.amazonaws:aws-java-sdk-bundle:1.12.262  /opt/bitnami/spark/jobs/spark-city.py
docker-compose exec spark-master spark-submit --master spark://spark-master:7077 --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,io.delta:delta-core_2.12:2.4.0  /opt/bitnami/spark/jobs/spark-city.py
7)

docker-compose down --> to delete the container